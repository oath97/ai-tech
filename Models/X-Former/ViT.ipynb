{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ebff5dd",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) 从零开始实现\n",
    "论文：[An Image is Worth 16X16 Words: Transformers for Image Recongnition at Scale](https://arxiv.org/pdf/2010.11929)\n",
    "\n",
    "## 网络结构\n",
    "<img src=\"./img/ViT-1.png\" width = \"600\" height = \"400\" alt=\"ViT 网络结构\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ca367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d2167",
   "metadata": {},
   "source": [
    "## Split Image\n",
    "<img src=\"./img/ViT-2.png\" width = \"600\" height = \"300\" align=center />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c342b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1., 12., 13.],\n",
       "         [ 2.,  3., 14., 15.],\n",
       "         [ 4.,  5., 16., 17.],\n",
       "         [ 6.,  7., 18., 19.],\n",
       "         [ 8.,  9., 20., 21.],\n",
       "         [10., 11., 22., 23.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Patches(nn.Module):\n",
    "    '''图像分割'''\n",
    "    def __init__(self, patch_h, patch_w, **kwargs):\n",
    "        super(Patches, self).__init__(**kwargs)\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "    \n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        args:\n",
    "            images:     tensor (batch_size, channels, height, width)\n",
    "        return:\n",
    "            patches:    tensor (batch_size, num_patches, channels * patch_size * patch_size)\n",
    "        '''\n",
    "        batch, channels, height, width = images.shape\n",
    "        \n",
    "        num_h = height // self.patch_h\n",
    "        num_w = width // self.patch_w\n",
    "        num_patches = num_h * num_w\n",
    "        \n",
    "        images = images.reshape(batch, channels, num_h, self.patch_h, num_w, self.patch_w)\n",
    "        images = images.permute(0, 2, 4, 1, 3, 5) # 会改变顺序\n",
    "        patches = images.reshape(batch, num_patches, channels, self.patch_h, self.patch_w)\n",
    "        patches = patches.reshape(batch, num_patches, -1)\n",
    "        \n",
    "        return patches\n",
    "\n",
    "  \n",
    "images = torch.arange(24, dtype=torch.float).reshape(1, 2, 3, 4) # B, C, H, W\n",
    "patches_module = Patches(1, 2)\n",
    "patches_module.eval()\n",
    "patches_1 = patches_module(images)\n",
    "patches_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a6b85",
   "metadata": {},
   "source": [
    "## Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4222489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearProjection(nn.Module):\n",
    "    '''Patch 线形投影'''\n",
    "    def __init__(self, input_dim, projection_dim, **kwargs):\n",
    "        \n",
    "        super(LinearProjection, self).__init__(**kwargs)\n",
    "        \n",
    "        self.projection = nn.Linear(input_dim, projection_dim)\n",
    "    \n",
    "    def forward(self, patches):\n",
    "        '''\n",
    "        args:\n",
    "            patches: tensor (batch_size, num_patches, input_dim)\n",
    "        return:\n",
    "            tensor (batch_size, num_patches, projection_dim)\n",
    "        '''\n",
    "        return self.projection(patches)\n",
    "\n",
    "proj_module = LinearProjection(4, 10)\n",
    "proj_module.eval()\n",
    "patches_2 = proj_module(patches_1)\n",
    "patches_2.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0d1ed",
   "metadata": {},
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30a80173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1.5023,   4.7399,   2.4235,  -7.3212,   3.8686,   1.4247,   1.0773,\n",
       "            5.7908,  -1.7574,   1.4652],\n",
       "         [  2.3151,   5.2620,   3.9575,  -8.3688,   4.9957,   1.6796,   1.5285,\n",
       "            6.7154,  -0.4849,   2.1983],\n",
       "         [  3.0486,   5.8617,   5.5067,  -9.4144,   6.1236,   1.8503,   1.9986,\n",
       "            7.6157,   0.8165,   2.9525],\n",
       "         [  3.8512,   6.3575,   7.0050, -10.4415,   7.2433,   2.0758,   2.4740,\n",
       "            8.4977,   2.1733,   3.6683],\n",
       "         [  4.5799,   6.9025,   8.5117, -11.5410,   8.3848,   2.3060,   2.9845,\n",
       "            9.3924,   3.4485,   4.4218],\n",
       "         [  5.3206,   7.4151,  10.0249, -12.5665,   9.4877,   2.5473,   3.4275,\n",
       "           10.3151,   4.7673,   5.1656]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    '''为输入序列添加可学习的位置嵌入'''\n",
    "    def __init__(self, max_seq_len=2048, **kwargs):\n",
    "        super(PositionEmbedding, self).__init__(**kwargs)\n",
    "        self.max_seq_len=max_seq_len\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        args:\n",
    "            inputs: tensor (batch_size, seq_len, hidden_dim)\n",
    "        return:\n",
    "            tensor (batch_size, seq_len, hidden_dim)\n",
    "        '''\n",
    "        _, seq_len, hidden_dim = inputs.shape\n",
    "        \n",
    "        # 动态创建位置嵌入表（如果不存在）\n",
    "        if not hasattr(self, 'position_embedding'):\n",
    "            # 初始化为可学习参数\n",
    "            self.position_embedding = nn.Parameter(torch.zeros(1, self.max_seq_len, hidden_dim))\n",
    "            nn.init.normal_(self.position_embedding, std=0.02)\n",
    "        \n",
    "        pos_emb = self.position_embedding[:, :seq_len]\n",
    "        \n",
    "        return inputs + pos_emb\n",
    "\n",
    "\n",
    "pos_module = PositionEmbedding()\n",
    "pos_module.eval()\n",
    "patches_3 = pos_module(patches_2)\n",
    "patches_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "451ac2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClassToken(nn.Module):\n",
    "    '''每个输入序列前添加可学习的分类令牌'''\n",
    "    def __init__(self, hidden_dim, **kwargs):\n",
    "        super(ClassToken, self).__init__(**kwargs)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        args:\n",
    "            inputs: tensor (batch_size, seq_len, hidden_dim)\n",
    "        return:\n",
    "            tensor (batch_size, seq_len + 1, hidden_dim)\n",
    "        '''\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        # 广播分类令牌到每一个batch\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        \n",
    "        return torch.cat([cls_tokens, inputs], dim = 1)\n",
    "\n",
    "cls_module = ClassToken(10)\n",
    "cls_module.eval()\n",
    "patches_4 = cls_module(patches_3)\n",
    "patches_4.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
